<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>ManipDreamer: Boosting Robotic Manipulation World Model with Action Tree and Visual Guidance</title>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>

<body>
    <!-- Title and Author -->
    <section class="hero is-medium">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-2 publication-title">ManipDreamer: Boosting Robotic Manipulation World Model with Action Tree and Visual Guidance</h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a target="_blank" href="https://github.com/myendless1">Ying&#160;Li</a><sup>1,3</sup>,
                                <a target="_blank" href="https://ucwxb.github.io/">Xiaobao&#160;Wei</a><sup>1,3</sup>,
                                <a target="_blank" href="https://github.com/litwellchi">Xiaowei&#160;Chi</a><sup>2</sup>,
                                <a target="_blank" href="https://github.com/Fredreic1849">Yuming&#160;Li</a><sup>1</sup>,
                                <a target="_blank" href="https://github.com/zhongyu-zhao">Zhongyu&#160;Zhao</a><sup>1</sup>,
                                <br>
                                <a target="_blank" href="https://pkuhaowang.github.io/">Hao&#160;Wang</a><sup>1</sup>,
                                <a target="_blank" href="https://nmaac.github.io/">Ningning&#160;Ma</a><sup>3</sup>,
                                <a target="_blank" href="https://lu-m13.github.io/">Ming&#160;Lu</a><sup>1</sup>,
                                <a target="_blank" href="https://www.shanghangzhang.com/">Shanghang&#160;Zhang</a><sup>1&#9993;</sup>
                            </span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block"><sup>1</sup>State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University;</span>
                            <span class="author-block"><sup>2</sup>Hong Kong University of Science and Technology;</span>
                            <span class="author-block"><sup>3</sup>Autonomous Driving Development, NIO;</span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block"><sup>&#9993;&#160;</sup>Corresponding Author</span>
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <span class="link-block">
                                    <a target="_blank" href="https://arxiv.org/pdf/2504.09540" class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon"><i class="ai ai-arxiv"></i></span>
                                        <span>arXiv</span>
                                    </a>
                                </span>

                                <span class="link-block">
                                    <a target="_blank" href="https://arxiv.org/pdf/2504.09540" class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon"><i class="fas fa-file-pdf"></i></span>
                                        <span>PDF</span>
                                    </a>
                                </span>

                                <span class="link-block">
                                    <a href="https://github.com/PKUHaoWang/EmbodiedOcc2" class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon"><i class="fab fa-github"></i></span>
                                        <span>Code coming soon</span>
                                    </a>
                                </span>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Abstract -->
    <section class="section">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3 has-text-centered">Abstract</h2>
                    <div class="content has-text-justified">
                        <p class="is-size-5">
                            While recent advancements in robotic manipulation video synthesis have shown promise, significant challenges persist in ensuring effective instruction-following and achieving high visual quality.
                            Recent methods, like RoboDreamer, utilize linguistic decomposition to divide instructions into separate lower-level primitives. 
                            They condition the world model on these primitives to achieve compositional instruction-following. 
                            However, these separate primitives do not take into account the relationships that exist between them. 
                            Furthermore, recent methods neglect valuable visual guidance, including depth and semantic guidance, both crucial for enhancing visual quality.
                            This paper introduces ManipDreamer, an advanced world model based on the action tree and visual guidance. 
                            To better learn the relationships between instruction primitives, we represent the instruction as the action tree and assign embeddings to tree nodes. 
                            Therefore, each instruction can acquire its embeddings by navigating through the action tree. 
                            The instruction embeddings can be used to guide the world model. 
                            To enhance visual quality, we combine depth and semantic guidance by introducing a visual guidance adapter compatible with the world model. 
                            This visual adapter enhances both the temporal and physical consistency of video generation.
                            Based on the action tree and visual guidance, ManipDreamer significantly boosts the instruction-following ability and visual quality.
                            Comprehensive evaluations on robotic manipulation benchmarks reveal that ManipDreamer achieves large improvements in video quality metrics in both seen and unseen tasks, 
                            with PSNR improved from 19.55 to 21.05, SSIM improved from 0.7474 to 0.7982 and reduced Flow Error from 3.506 to 3.201 in unseen tasks, compared to the recent RoboDreamer model.
                            Additionally, our method increases the success rate of robotic manipulation tasks by 2.5% in 6 RLbench tasks on average.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Motivation -->
    <section class="section">
        <div class="container is-max-desktop">
            <div class="is-centered has-text-centered">
                <div class="is-four-fifths">
                    <h2 class="title is-3">Motivation</h2>
                    <div class="content">
                        <img src="assets/images/challenges.jpg" class="interpolation-image" alt="" style="max-width: 80%; margin: 2rem auto; display: block;" />
                        <p class="is-size-5 has-text-justified">
                            There are multiple common defects in robotic video generation, including 
                            <span style="color: #E4A902;">
                                instruction misalignment, hallucinations, spatial errors, duplicate objects, temporal discontinuities, and failed executions. 
                            </span> 
                            This figure presents sample comparisons between RoboDreamer and our proposed ManipDreamer, illustrating the effectiveness of our proposed approach.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Method Overview -->
    <section class="section">
        <div class="container is-max-desktop">
            <div class="has-text-centered">
                <h2 class="title is-3">Method Overview</h2>
                <div class="content">
                    <img src="assets/images/main-method.jpg" alt="Method Overview" style="max-width: 90%; margin: 2rem auto; display: block;" />
                    <p class="is-size-5 has-text-justified">
                        Overview of ManipDreamer: 
                        We encode language instructions as verb-preposition action trees to capture compositional task structure (a) 
                        and inject depth and semantic features through a hierarchical control adapter (b) to enhance spatial-temporal consistency in video generation. 
                        In the UNet decoder, action tree embeddings and visual guidance features are sequentially injected every 3 layers using cross-attention mechanisms(c).
                    </p>
                </div>

                <div class="content has-text-justified">
                    <h2 class="title is-4">Highlights</h2>
                    <ul class="is-size-5">
                        <li>
                            We propose an advanced world model ManipDreamer for robotic manipulation that ensures <span style="color: #E4A902;">effective instruction-following</span> and achieves <span style="color: #E4A902;">high visual quality</span>.
                        </li>
                        <li>
                            We propose representing the <span style="color: #E4A902;">instruction as an Action Tree </span> to learn the relationships between instruction primitives.
                        </li>
                        <li>
                            We enhance visual quality with a well-designed <span style="color: #E4A902;">Multi-modal Control Adapter</span> that integrates Depth and Semantics into robotic world models through hierarchical guidance.
                        </li>
                    </ul>
                </div>
            </div>
        </div>
    </section>

    <!-- Visualization comparison -->
    <section class="section">
        <div class="container is-fluid">
            <div class="has-text-centered">
                <h2 class="title is-3">Visualization Comparison(partial)</h2>
                <div class="columns is-centered" style="margin-bottom: 2rem;">
                    <div class="column is-6">
                        <div class="is-flex is-align-items-center" style="margin-bottom: 1rem;">
                            <span class="is-size-5" style="margin-right: 1rem;">Task:</span>
                            <div class="select is-medium">
                                <select id="task-select" onchange="updateImageSelect()">
                                    <option value="knock redbull can over" selected>knock redbull can over</option>
                                </select>
                            </div>
                        </div>
                        <div class="is-flex is-align-items-center">
                            <span class="is-size-5" style="margin-right: 1rem;">Example:</span>
                            <div class="select is-medium">
                                <select id="image-select" onchange="updateVisualization()">
                                    <option value="0069.png" selected>0069</option>
                                </select>
                            </div>
                        </div>
                    </div>
                </div>

                <div class="columns is-centered">
                    <div class="column is-10">
                        <div class="columns">
                            <div class="column is-1" style="display: flex; flex-direction: column; justify-content: center;">
                                <div style="margin-bottom: 70%;">
                                    <span class="is-size-5 has-text-weight-bold">RoboDreamer</span>
                                </div>
                                <div>
                                    <span class="is-size-5 has-text-weight-bold">ManipDreamer(Ours)</span>
                                </div>
                            </div>
                            <div class="column is-11">
                                <img id="visualization-image" src="assets/more_vis/knock redbull can over/0069.png" class="interpolation-image" alt="" style="max-width: 60%; margin: 0 auto; display: block;" />
                            </div>
                        </div>
                    </div>
                </div>

                <div class="content has-text-justified" style="max-width: 800px; margin: 2rem auto;">
                    <p class="is-size-5">
                        These generated images show the effectiveness of our proposed two novel methods: Action Tree and Multi-modal Control Adapter, 
                        enhancing the action motion consistency and relieve the defects in the generated videos.
                    </p>
                </div>
            </div>
        </div>
    </section>

    <footer class="footer">
        <div class="container">
            <div class="columns is-centered">
                <div class="column is-8">
                    <div class="content has-text-centered">
                        <p class="is-size-6">
                            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
                            You are free to borrow the of this website, we just ask that you link back to this page in the footer.
                            <br>
                            This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </footer>

    <script>
        // 静态的任务和图片字典
        const taskImages = {
            "knock redbull can over": ["0069.png", "0086.png", "0114.png"],
            "place pepsi can into top drawer": ["0253.png"],
            "place orange can into middle drawer": ["0469.png"],
            "place brown chip bag into bottom drawer": ["0060.png", "0099.png"],
            "place apple into bottom drawer": ["0320.png", "0336.png", "0384.png", "0417.png"],
            "pick rxbar chocolate": ["0091.png", "0157.png", "0223.png"],
            "pick 7up can": ["0041.png", "0085.png", "0258.png", "0275.png", "0290.png", "0374.png"],
            "move rxbar chocolate near brown chip bag": ["0164.png"],
            "move coke can near apple": ["0123.png", "0790.png", "0810.png", "0834.png"]
        };

        // 更新任务选择器选项
        function updateTaskSelect() {
            const select = document.getElementById('task-select');
            
            // 清空现有选项（保留默认选项）
            while (select.options.length > 1) {
                select.remove(1);
            }
            
            // 添加新选项
            Object.keys(taskImages).forEach(task => {
                if (task !== 'knock redbull can over') {
                    const option = document.createElement('option');
                    option.value = task;
                    option.text = task;
                    select.add(option);
                }
            });
        }

        // 更新图片选择器选项
        function updateImageSelect() {
            const taskSelect = document.getElementById('task-select');
            const imageSelect = document.getElementById('image-select');
            const selectedTask = taskSelect.value;
            const images = taskImages[selectedTask];
            
            // 清空现有选项
            imageSelect.innerHTML = '';
            
            // 添加新选项
            images.forEach(image => {
                const option = document.createElement('option');
                option.value = image;
                option.text = image.replace('.png', '');
                if (image === '0069.png' && selectedTask === 'knock redbull can over') {
                    option.selected = true;
                }
                imageSelect.add(option);
            });

            // 更新图片显示
            updateVisualization();
        }

        function updateVisualization() {
            const taskSelect = document.getElementById('task-select');
            const imageSelect = document.getElementById('image-select');
            const image = document.getElementById('visualization-image');
            image.src = `assets/more_vis/${taskSelect.value}/${imageSelect.value}`;
        }

        // 页面加载完成后更新选项
        document.addEventListener('DOMContentLoaded', () => {
            updateTaskSelect();
            updateImageSelect();
        });
    </script>
</body>

</html>